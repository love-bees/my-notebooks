{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stats.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOYTz78Tlp5HQyRM+KYWIXn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/love-bees/my-notebooks/blob/master/Stats.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rR3SzrTMPP0",
        "colab_type": "text"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IVg2ailWElI",
        "colab_type": "text"
      },
      "source": [
        "## Structured data elements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul8Fdi0qMiIN",
        "colab_type": "text"
      },
      "source": [
        "Data can come from many different sources and it is often unstructured:\n",
        "* Sensors\n",
        "* Events (such as clickstreams)\n",
        "* Images (collection of RGB pixels)\n",
        "* Videos\n",
        "* Text (sequences of words and non word characters)\n",
        "\n",
        "The greatest challange in Data Science is to collect and process this data into a usable structured format. A common form of structured data is a *table* with *rows* and *columns*. \n",
        "\n",
        "There are two types of strucutred data:\n",
        "* Numeric - numeric scale\n",
        "    * Continuous - takes on any value in an interval (*Synonyms*: float, interval)\n",
        "    * Discrete - takes on integer values (*Synonyms*: integer, count)\n",
        "* Categorical - specific set of values which represent a set of possible categories \n",
        "    * Binary - Special case of just two categories of values, 1/0 or True/False (*Synonyms*: boolean, logical)\n",
        "    * Ordinal - categorical data with specific ordering.\n",
        "\n",
        "Knowing the type of data and the structures you're dealing with can act as a signal for statistical procedures, such as fitting a model or plotting the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7V8c536WIgN",
        "colab_type": "text"
      },
      "source": [
        "## Rectangular data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9HJVOA1WN8m",
        "colab_type": "text"
      },
      "source": [
        "Spreadsheets or database tables are typical reference points for data analysis. *Rectangular data* is represented by a two-dimensional matrix with *records* (cases) and *features* (variables). The format of this kind of data in Python is a *dataframe*. However, it doesn't always start out this way: unstructured data needs to be processed to form a set of features in rectangular data. Data in databases needs to be extracted and formatted into a single table.\n",
        "\n",
        "* Dataframe - rectangular data is the basic data structure for statistical and machine learning models (*Synonyms*: spreadsheet, database)\n",
        "* Feature - a column within a table (*Synonyms*: variable, input, predictor)\n",
        "* Outcome - Often a yes/no prediction which sometimes uses *features* (*Synonyms*: dependent variable, output, target, response)\n",
        "* Records - Rows within a table (*Synonyms*: case, sample, instance, example, observation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He-J4mjjZOdX",
        "colab_type": "text"
      },
      "source": [
        "### Indexes and Dataframes\n",
        "\n",
        "Database tables have one (or more) columns called *index* and it is typically a row number. In Python, Dataframes automatically add an integer index based on the order of rows. In `pandas` you can also set multilevel indexing to improve the efficiency of certain problems.\n",
        "\n",
        "When comparing statistics and data science, the terminology for rectangular data can get confusing. Sometimes they use different terms to describe the same thing, while there's also the same term to describe two different things.\n",
        "\n",
        "For a statistician, *predictor variables* are used to predict a *resposne* or a *dependent variable* in a model. A *sample* is a collection of rows.\n",
        "\n",
        "For a data scientist, *features* are used to predict a *target* in a model. A *sample* is a single row."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naPQtkbHat9O",
        "colab_type": "text"
      },
      "source": [
        "### Non rectangular Data Structures\n",
        "\n",
        "There are other data structures besides rectangular. \n",
        "\n",
        "* *Time series data structures* - successive measurements of the same variable. It's a key component that's produced by IoT devices, as well as raw material for statistical forecasting methods. \n",
        "* *Spacial data structures* - used in mapping and location analysis and it is more complex than rectangular data structures. \n",
        "    * *Object representation* - a specific object (house, building) and it's spacial coordinates\n",
        "    * *Field view* - small units of space and the value of a relevant measurement (brightness, color)\n",
        "* *Graph (network) data structures* - used for representing abstract, physical and social relationships. Used for recommendation systems and network optimization. In statistics, a graph is a variety of plots and visualizations. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9Ey7xPobUwp",
        "colab_type": "text"
      },
      "source": [
        "## Estimates of Location"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQcjcknxcyRh",
        "colab_type": "text"
      },
      "source": [
        "* Mean - sum of all values divided by the number of values (*Synonym*: average)\n",
        "* Weighted mean - sum of all values times a weight divided by the sum of weights (*Synonym*: weighted average)\n",
        "* Trimmed mean - Average of all values after dropping a fixed number of extreme values (*Synonym*: turnicated mean)\n",
        "* Median - value where half of the data lies above and half of the data below it (*Synonyms*: 50th percentile)\n",
        "* Weighted median - Value such that half of the sum of weights lies above and below the sorted data\n",
        "* Percentile - Value such that a percent of the data lies below the value (*Synonym*: quantile)\n",
        "* Robust - not sensitive to extreme values (*Synonym*: resistant)\n",
        "* Outlier - A value that is significantly different from most of the data (*Synonyms*: extreme value)\n",
        "\n",
        "A mere mean might not tell you what it all means, you know what I mean? :) Means are easy to calculate, but they might not be the best measure of a central value. This is why statisticians have developed a number of alternative estimates to the mean."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31K7bpclftjo",
        "colab_type": "text"
      },
      "source": [
        "### Mean\n",
        "\n",
        "This is the most basic estimate of location. It's also called the *average*. A mean is the sum of all values divided by the number of values. Consider the following example set: $ \\{2, 5, 7, 10\\} $. The mean would be: $\\bar{x} = \\frac{2+5+7+8}{4} = \\frac{22}{4} = 5.5$. The formula for computing a mean on a set of $n$ values $x_1, x_2, ..., x_n$ is:\n",
        "\n",
        "$\\bar{x} = \\frac{\\sum_{i=1}^{n}x_i}{n}$\n",
        "\n",
        "In statistics, you might also come accross $N$ in the formula. Uppercase denotes the mean over the whole set of data while the lowercase is computed on a sample of the data. In data science this distinction is not as important.\n",
        "\n",
        "#### Trimmed mean\n",
        "\n",
        "A variation of the regular mean, it represents the average of all values after dropping a fixed numebr of extreme values. Taking the sorted values in a set $ x_{(1)}, x_{(2)}, ..., x_{(n)}$, the formula to compute the trimmed mean after dropping $p$ smallest and largest values is:\n",
        "\n",
        "$\\bar{x} = \\frac{\\sum_{i=p+1}^{n-p}x_{(i)}}{n - 2p}$\n",
        "\n",
        "Trimmed mean obviusly eliminates the influence of extreme values. In many cases, a trimmed mean is preferable to a regular mean.\n",
        "\n",
        "\n",
        "#### Weighted mean\n",
        "\n",
        "Another type of mean is the weighted mean. It is calculated by multiplying every value by a specified weight $w_i$ and dividing by the sum of weights. The formula looks like this:\n",
        "\n",
        "$\\bar{x}_w = \\frac{\\sum_{i=1}^{n}w_i x_i}{\\sum_{i=1}^{n}w_i}$\n",
        "\n",
        "Two main reasons why this is used:\n",
        "1. Some data is more variable than other data. For example, if we have a less accurate then another sensor, we want to set the weight to a low value for the less accurate sensor.\n",
        "2. Data might not equally represent all groups we are interested in measuring and analyzing. For that reason, we would set a high weight for underrepresented data in order to give it a higher value.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ypm9MZ1fvhV",
        "colab_type": "text"
      },
      "source": [
        "### Median and Robust Estimates\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsPJ1tn9nBdS",
        "colab_type": "text"
      },
      "source": [
        "A median is the middle number in a sorted list. If the list has an even number of elements, then the number is not actually in the list. You would take the average of the two middle values. Compared to the mean, which looks at the data as a whole, the median only looks at the middle values of the data. While this might seem like a disadvantage, it actually could be an advantage and a better metric for location. The mean can be quite sensitive in comparison to the median.\n",
        "\n",
        "For exmple, we can compare household incomes in two areas. If Mark Zuckerberg lives in one of those areas, the mean income would be high for his area. However, if we use the median then it won't matter in which area Mr. Zuckerberg lives. The position of the middle area will remain the same and provide a better consistency.\n",
        "\n",
        "You may use weighted median for the same reasons you would use a weighted mean. You would first sort the data and each data will have an associated weight with it. Instead of taking the middle number, you would find the sum of the weights such that the upper and lower halves are equal. Weighted median is more robust to outliers as well.\n",
        "\n",
        "#### Outliers\n",
        "\n",
        "The median is usually refered to as the robust estimate of location because it's not affected by outliers, which could skew the results. But what is an outlier?\n",
        "\n",
        "An outlier is any value that is very distant from the rest of the dataset. How distant is a matter of opinion, although some conventions do exist. Being an outlier doesn't necessarely make the data value invalid (as with the previous example with Mark Zuckerberg, he tehnically does have a large income). Often times outliers are indeed errors though. It might be a bad measurement from a sensor or a unit error. Outliers should always be identified and further examined because they can give a poor estimate of a location.\n",
        "\n",
        "Median is a great way to deal with outliers, but it's not the only robust estimate of location. Trimmed mean is also widely used to avoid the influence of outliers on the mean. A common choice is to trim the top and bottom 10% of data and it provides a great protection against outliers in datasets which are larger. The trimmed mean is usually the prefect choice because you get the best of both worlds: using larger amounts of data while also getting rid of outliers.\n",
        "\n",
        "## Example:\n",
        "\n",
        "Let's see how we can calculate all these based on some realistic data about murder rates in USA by state in 2010. You can get the data from this [GitHub](https://github.com/gedeck/practical-statistics-for-data-scientists/tree/master/data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy9Cjn5PvQi9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Kpd1J05vR76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Source: \n",
        "https://github.com/gedeck/practical-statistics-for-data-scientists/tree/master/data\n",
        "under data tables, state.csv\n",
        "\"\"\"\n",
        "data = pd.read_csv('state.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FrqxBeJvYY-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "63b37047-9cb3-4c5f-a38e-d2b78ac1ed2f"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>State</th>\n",
              "      <th>Population</th>\n",
              "      <th>Murder.Rate</th>\n",
              "      <th>Abbreviation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Alabama</td>\n",
              "      <td>4779736</td>\n",
              "      <td>5.7</td>\n",
              "      <td>AL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Alaska</td>\n",
              "      <td>710231</td>\n",
              "      <td>5.6</td>\n",
              "      <td>AK</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Arizona</td>\n",
              "      <td>6392017</td>\n",
              "      <td>4.7</td>\n",
              "      <td>AZ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Arkansas</td>\n",
              "      <td>2915918</td>\n",
              "      <td>5.6</td>\n",
              "      <td>AR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>California</td>\n",
              "      <td>37253956</td>\n",
              "      <td>4.4</td>\n",
              "      <td>CA</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        State  Population  Murder.Rate Abbreviation\n",
              "0     Alabama     4779736          5.7           AL\n",
              "1      Alaska      710231          5.6           AK\n",
              "2     Arizona     6392017          4.7           AZ\n",
              "3    Arkansas     2915918          5.6           AR\n",
              "4  California    37253956          4.4           CA"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDqV92C0PI3J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "19e9ac05-3333-454f-8c0c-07fc06097b4a"
      },
      "source": [
        "from scipy.stats import trim_mean\n",
        "import numpy as np\n",
        "import wquantiles\n",
        "\n",
        "\n",
        "print('Mean population:', data['Population'].mean())\n",
        "print('Trimmed mean population:',trim_mean(data['Population'],0.1))\n",
        "print('Median:',data['Population'].median())\n",
        "\n",
        "print('\\nMean murder rate:', data['Murder.Rate'].mean())\n",
        "print('Weighted mean murder rate:', np.average(data['Murder.Rate'], weights=data['Population']))\n",
        "print('Weighted median murder rate:', wquantiles.median(data['Murder.Rate'], weights=data['Population']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean population: 6162876.3\n",
            "Trimmed mean population: 4783697.125\n",
            "Median: 4436369.5\n",
            "\n",
            "Mean murder rate: 4.066\n",
            "Weighted mean murder rate: 4.445833981123393\n",
            "Weighted median murder rate: 4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nFFlZ6Rbx69",
        "colab_type": "text"
      },
      "source": [
        "## Estimates of variability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5K2-brub3LB",
        "colab_type": "text"
      },
      "source": [
        "Variability or *dispersion* is the second dimension of data analysis that we can use to summarize a feature. It measures whether the data points are tightly clustered or widely spread out. A lot of statistical decisions depend on the estimate of variability. We can measure, reduce, distinguish random from real, identify the sources of the variability and make decisions based on those factors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5H2IVerb0Jr",
        "colab_type": "text"
      },
      "source": [
        "* Deviations - difference between observed value and the estimate of location (errors, residuals).\n",
        "* Variance - sum of squared deviations from the mean divided by $n-1$ where $n$ is the number of data values (mean squared error).\n",
        "* Standard deviation - square root of the variance.\n",
        "* Mean absolute deviation - mean of the absolute values of the deviations from the mean (l1-norm, Manhattan norm).\n",
        "* Median absolute deviation from the median - median of the absolute values of the deviations from the median.\n",
        "* Range - difference between smallest and largest value in the dataset.\n",
        "* Order statistics - metrics based on values sorted from smallest to largest (ranks).\n",
        "* Percentile - value such that $P$ percent of the values take on this value or less and $(100-P)$ take on this value or more (quantile).\n",
        "* Interquartile range - difference between the 75th percentile and 25th percentile (IQR)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQiSy-A9eI3S",
        "colab_type": "text"
      },
      "source": [
        "Same way you can measure the location in different ways, there are also different ways of measuring the variability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCdC87BZelbc",
        "colab_type": "text"
      },
      "source": [
        "### Standard deviation and related estimates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lK3DRgseqfO",
        "colab_type": "text"
      },
      "source": [
        "The most widely used estimate of variability is based on differences, or *deviations* between the estimates of location and the observed data. For example, let's take a set of data points $\\{1, 2, 6\\}$. The mean is $3$. The deviations from the mean are the differences: $1-3=-2$, $2-3 = -1$ and $6-3=3$. These deviations tell us how dispersed the data is around the mean. The average of these won't tell us much because the negatives cancel out the positives. The total sum is exactly 0. Instead, the simplest approach is to take the average of the absolute values. In our example, the sum of the absolute values would be $6$ and the average would be $6/3 = 2$ This is the *mean absolute deviation*. The formula is as follows:\n",
        "\n",
        "$\\text{Mean absolute deviation} = \\frac{\\sum_{i=1}^{n}|{x_i-\\bar{x}}|}{n}$\n",
        "\n",
        "Where $\\bar{x}$ is the mean of the sample.\n",
        "\n",
        "The most well known estimates of variability are the *variance* and *standard deviation*. The variance is the average of the squared deviations and the standard deviation is the square root of the variance.\n",
        "\n",
        "$\\text{Variance} = s^2 = \\frac{\\sum_{i=1}^{n}({x_i-\\bar{x}})^2}{n-1}$\n",
        "\n",
        "$\\text{Standard deviation} = s = \\sqrt{\\text{Variance}} = \\sqrt{\\frac{\\sum_{i=1}^{n}({x_i-\\bar{x}})^2}{n-1}}$\n",
        "\n",
        "The standard deviation is easier to interpret than the variance. This is because standard deviation is on the same scale as our data. Even though the formula is less intuitive, it's prefered when compared to mean absolute deviation. The idea comes from statistical theory in mathematics: working with squared values is much more convenient than with absolute values when it comes to statistical models.\n",
        "\n",
        "You might be wondering why the formula uses $n-1$ instead of $n$. This stems from a concept in statistics called *degrees of freedom*. Most of the time $n$ will be large enough that subctracting 1 from it won't make much of a difference. \n",
        "\n",
        "Here's the backstory anyways: you start with a premise that you're making estimates on a sample from a population. If you take the intuitive denominator $n$ you will underestimate the true value of the variance and standard deviation. This is called a *biased* estimate. However, if you take the less intuitive $n-1$ as the denominator, then you will end up with an *unbiased* estimate. Why does this happen though? Degrees of freedom takes into account the number of values that can be changed, or variables. In the case of variance and standard deviation, we have 1 variable and that is the sample mean. \n",
        "\n",
        "However, for most problems, data scientist don't need to worry about this.\n",
        "\n",
        "Mean absolute deviation (or error), variance and standard deviation don't take into account outliers and extreme values. Variance and standard deviation are especially sensitive to outliers because the deviation is squared. However, there is a robust estimate of variability. It's called the Median Aboslute Deviation From the Median, or MAD:\n",
        "\n",
        "$\\text{MAD} = \\text{Median}(|x_1-m|, |x_2-m|, ... , |x_n-m|)$\n",
        "\n",
        "Where $m$ is the median. Just like medians, MAD is more robust to outliers. We can also compute a trimmed standard deviation analogous to the trimmed mean.\n",
        "\n",
        "It's important to note that the variance, standard deviation, mean absolute deviation and median absolute deviation from the median are all different estimates and will give different values. Standard deviation is always the greatest, then comes the mean absolute deviation, then the median absolute deviation. The mean absolute deviation is sometimes multiplied by a factor in order to normalize it to the standard deviation scale. For a normal distribution, this factor is 1.4826 and it means that 50% of the distribution will fall into the $\\pm\\text{MAD}$ range."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN9sddHDhsu-",
        "colab_type": "text"
      },
      "source": [
        "### Estimates based on percentiles\n",
        "\n"
      ]
    }
  ]
}